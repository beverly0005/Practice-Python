{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://towardsdatascience.com/build-nlp-pipelines-with-huggingface-datasets-d597ff5f68ad\n",
    "\n",
    "The essay talks about the good datasets for NLP projects provided by HuggingFace (HF). There are 718 datasets available so far, and HF develops an app to view them: https://huggingface.co/datasets/viewer/\n",
    "\n",
    "This script is just a start to look into these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 670 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from datasets) (0.3.1.1)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-macosx_10_9_x86_64.whl (570 kB)\n",
      "\u001b[K     |████████████████████████████████| 570 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from datasets) (1.21.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from datasets) (2.22.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py37-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from datasets) (1.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: packaging in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from datasets) (21.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow!=4.0.0,>=3.0.0\n",
      "  Downloading pyarrow-6.0.1-cp37-cp37m-macosx_10_13_x86_64.whl (19.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.1 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.1.1)\n",
      "Requirement already satisfied: filelock in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from packaging->datasets) (2.4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp37-cp37m-macosx_10_9_x86_64.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 5.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-macosx_10_9_x86_64.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from aiohttp->datasets) (19.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-macosx_10_9_x86_64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from importlib-metadata->datasets) (0.5.1)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 2.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from pandas->datasets) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from pandas->datasets) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhiyuwang/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, charset-normalizer, asynctest, async-timeout, aiosignal, tqdm, fsspec, dill, aiohttp, xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.43.0\n",
      "    Uninstalling tqdm-4.43.0:\n",
      "      Successfully uninstalled tqdm-4.43.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 0.17.1\n",
      "    Uninstalling pyarrow-0.17.1:\n",
      "      Successfully uninstalled pyarrow-0.17.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.12\n",
      "    Uninstalling huggingface-hub-0.0.12:\n",
      "      Successfully uninstalled huggingface-hub-0.0.12\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scispacy 0.2.4 requires awscli, which is not installed.\n",
      "transformers 4.9.2 requires huggingface-hub==0.0.12, but you have huggingface-hub 0.1.2 which is incompatible.\n",
      "streamlit 0.70.0 requires botocore>=1.13.44, but you have botocore 1.12.253 which is incompatible.\n",
      "scispacy 0.2.4 requires spacy>=2.2.1, but you have spacy 2.1.8 which is incompatible.\n",
      "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.8 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 charset-normalizer-2.0.8 datasets-1.16.1 dill-0.3.4 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.1.2 multidict-5.2.0 multiprocess-0.70.12.2 pyarrow-6.0.1 tqdm-4.62.3 xxhash-2.0.2 yarl-1.7.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/zhiyuwang/anaconda3/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View List of Datasets Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = datasets.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 2028)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_list), len(ds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jinmang2/temp',\n",
       " 'kiyoung2/temp',\n",
       " 'Graphcore/wikipedia-bert-512',\n",
       " 's3h/customized-qalb-v2',\n",
       " 's3h/arabic-grammar-corrections']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_list[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GEM/squad_v2',\n",
       " 'Gabriel/squad_v2_sv',\n",
       " 'Serhii/Custom_SQuAD',\n",
       " 'Tevatron/wikipedia-squad-corpus',\n",
       " 'Tevatron/wikipedia-squad',\n",
       " 'Wikidepia/IndoSQuAD',\n",
       " 'adamlin/coqa_squad',\n",
       " 'dweb/squad_with_cola_scores',\n",
       " 'lhoestq/custom_squad',\n",
       " 'lhoestq/squad',\n",
       " 'lhoestq/squad_titles',\n",
       " 'lincoln/newsquadfr',\n",
       " 'philschmid/test_german_squad',\n",
       " 'piEsposito/squad_20_ptbr',\n",
       " 'qwant/squad_fr',\n",
       " 'shivmoha/squad-unanswerable',\n",
       " 'shivmoha/squad_adversarial_manual',\n",
       " 'susumu2357/squad_v2_sv',\n",
       " 'vershasaxena91/squad_multitask',\n",
       " 'z-uo/squad-it',\n",
       " 'iapp_wiki_qa_squad',\n",
       " 'squad',\n",
       " 'squad_adversarial',\n",
       " 'squad_es',\n",
       " 'squad_it',\n",
       " 'squad_kor_v1',\n",
       " 'squad_kor_v2',\n",
       " 'squad_v1_pt',\n",
       " 'squad_v2',\n",
       " 'squadshifts',\n",
       " 'thaiqa_squad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ds for ds in ds_list if 'squad' in ds.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the whole dataset directly, but if the dataset is too large, this may not be a good idea. An alternative way is to download the dataset iteratively by setting streaming = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset. Data is split into train and test, here only loading the train part\n",
    "data = datasets.load_dataset('squad', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.iterable_dataset.IterableDataset"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89789763\n",
      "@article{2016arXiv160605250R,\n",
      "       author = {{Rajpurkar}, Pranav and {Zhang}, Jian and {Lopyrev},\n",
      "                 Konstantin and {Liang}, Percy},\n",
      "        title = \"{SQuAD: 100,000+ Questions for Machine Comprehension of Text}\",\n",
      "      journal = {arXiv e-prints},\n",
      "         year = 2016,\n",
      "          eid = {arXiv:1606.05250},\n",
      "        pages = {arXiv:1606.05250},\n",
      "archivePrefix = {arXiv},\n",
      "       eprint = {1606.05250},\n",
      "}\n",
      "\n",
      "\n",
      "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
      "\n",
      "\n",
      "{'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(data.dataset_size )\n",
    "print(data.citation+ '\\n')\n",
    "print(data.description+ '\\n')\n",
    "print(data.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we load the whole data, then we may use data[0] to view the content, but if we use streaming = True, then we need to use the following loop to see it. Note: next() doesn't work here as 'IterableDataset' object is not an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"5733be284776f41900661182\",\n",
      "    \"title\": \"University_of_Notre_Dame\",\n",
      "    \"context\": \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \\\"Venite Ad Me Omnes\\\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\",\n",
      "    \"question\": \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\",\n",
      "    \"answers\": {\n",
      "        \"text\": [\n",
      "            \"Saint Bernadette Soubirous\"\n",
      "        ],\n",
      "        \"answer_start\": [\n",
      "            515\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(json.dumps(i, indent=4))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go over the data to turn it into a data frame or any format you are familiar, then start data processing. Or you can use the data processing function already avaialble for the HF data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Delete column\n",
    "data1 = data.remove_columns(['question'])\n",
    "print(data1.features)  # after this practice, data1 doesn't have the same properties as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous'], 'answer_end': [541]}}\n",
      "{'id': '5733be284776f41900661182', 'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous'], 'answer_end': [541]}, 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}\n"
     ]
    }
   ],
   "source": [
    "# Change and keep necessary columns only, use \"map\"\n",
    "# Example 1\n",
    "data1 = data.map(\n",
    "    lambda x: {\n",
    "        'answers': {\n",
    "            **x['answers'],\n",
    "            **{'answer_end': [x['answers']['answer_start'][0] + len(x['answers']['text'][0])]}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "for i in data1:\n",
    "    print(i)\n",
    "    break\n",
    "    \n",
    "# Example 2\n",
    "data2 = data.map(\n",
    "    lambda x: {\n",
    "        'id': x['id'],\n",
    "        'answers': {\n",
    "            **x['answers'],\n",
    "            **{'answer_end': [x['answers']['answer_start'][0] + len(x['answers']['text'][0])]}\n",
    "        },\n",
    "        'question': x['question'],\n",
    "        'title': x['title']\n",
    "    }\n",
    ")\n",
    "\n",
    "for i in data2:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
